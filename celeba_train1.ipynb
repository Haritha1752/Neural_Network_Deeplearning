{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648dd2f3-ed3d-4e01-baf5-6d9c392c213c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18\n",
      "Torch: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: Tesla P100-PCIE-12GB\n"
     ]
    }
   ],
   "source": [
    "import torch, sys\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77ecc5-6402-49dd-8ee8-0b5dc1505800",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow>=2.12.0 pandas numpy scikit-learn opencv-python pillow matplotlib seaborn tqdm h5py scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b296b3-0dc0-4b69-8512-f5a3af473499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed-set 50k split\n",
      "====================\n",
      "train: n=39998, ids=1666, min/max per id=23/28\n",
      "val  : n= 5001, ids=1666, min/max per id=3/4\n",
      "test : n= 5001, ids=1666, min/max per id=3/4\n",
      "TOTAL: 50000 | NUM_CLASSES: 1666\n",
      "\n",
      "Examples:\n",
      "      image_id  identity split3  label\n",
      "0  000003.jpg      8692  train   1412\n",
      "1  000006.jpg      4153  train    574\n",
      "2  000007.jpg      9040  train   1513\n",
      "3  000008.jpg      6369  train   1074\n",
      "4  000009.jpg      3332  train    469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_557672/1393807774.py:80: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subset = subset.groupby(\"identity\", group_keys=False).apply(split_one)\n"
     ]
    }
   ],
   "source": [
    "# --- Closed-set 50k split for identity classification (CelebA)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "ROOT    = Path(\"/home/anand.ha/NNDL/celeba\")\n",
    "RAW     = ROOT / \"data/raw\"\n",
    "IMG_DIR = RAW  / \"img_align_celeba\"\n",
    "ID_PATH = RAW  / \"identity_CElebA.csv\"  # accept both spellings\n",
    "if not ID_PATH.exists(): ID_PATH = RAW / \"identity_CelebA.csv\"\n",
    "\n",
    "assert IMG_DIR.is_dir(), f\"Images folder missing: {IMG_DIR}\"\n",
    "assert ID_PATH.exists(), f\"Identity file missing: {ID_PATH}\"\n",
    "\n",
    "def load_identity(path: Path) -> pd.DataFrame:\n",
    "    # 1) try CSV with header\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        cols = [c.strip().lower() for c in df.columns]\n",
    "        if {\"image_id\",\"identity\"}.issubset(cols):\n",
    "            df.columns = cols\n",
    "            return df[[\"image_id\",\"identity\"]]\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) CSV w/o header\n",
    "    try:\n",
    "        df2 = pd.read_csv(path, header=None).iloc[:, :2]\n",
    "        df2.columns = [\"image_id\",\"identity\"]\n",
    "        return df2\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 3) whitespace-separated\n",
    "    df3 = pd.read_csv(path, sep=r\"\\s+\", header=None, engine=\"python\").iloc[:, :2]\n",
    "    df3.columns = [\"image_id\",\"identity\"]\n",
    "    return df3\n",
    "\n",
    "id_df = load_identity(ID_PATH)\n",
    "id_df[\"image_id\"] = id_df[\"image_id\"].astype(str).str.strip()\n",
    "id_df[\"identity\"] = pd.to_numeric(id_df[\"identity\"], errors=\"coerce\")\n",
    "id_df = id_df.dropna(subset=[\"identity\"]).reset_index(drop=True)\n",
    "id_df[\"identity\"] = id_df[\"identity\"].astype(int)\n",
    "\n",
    "# keep only rows whose image actually exists\n",
    "exists = id_df[\"image_id\"].apply(lambda n: (IMG_DIR / n).is_file())\n",
    "id_df = id_df[exists].reset_index(drop=True)\n",
    "\n",
    "# identities with at least 3 images (so we can do train/val/test)\n",
    "counts = id_df[\"identity\"].value_counts()\n",
    "eligible_ids = counts[counts >= 3].index.tolist()\n",
    "\n",
    "TARGET = 50_000  # <<<<<<<< 50k total images\n",
    "\n",
    "# choose identities (most frequent first) until total images >= TARGET\n",
    "chosen_ids, total = [], 0\n",
    "for ident, c in counts.loc[eligible_ids].items():\n",
    "    chosen_ids.append(ident); total += c\n",
    "    if total >= TARGET: break\n",
    "\n",
    "subset = id_df[id_df[\"identity\"].isin(chosen_ids)].copy()\n",
    "\n",
    "# per-identity closed-set split ~80/10/10 with minimums\n",
    "def split_one(g):\n",
    "    n = len(g)\n",
    "    idx = rng.permutation(n)\n",
    "    n_val  = max(1, int(round(0.10*n)))\n",
    "    n_test = max(1, int(round(0.10*n)))\n",
    "    n_train = max(1, n - n_val - n_test)\n",
    "    if n == 3:   n_train, n_val, n_test = 1,1,1\n",
    "    elif n == 4: n_train, n_val, n_test = 2,1,1\n",
    "    elif n == 5: n_train, n_val, n_test = 3,1,1\n",
    "    tr = idx[:n_train]; va = idx[n_train:n_train+n_val]; te = idx[n_train+n_val:]\n",
    "    out = g.copy()\n",
    "    out.loc[out.index.isin(g.index[tr]), \"split3\"] = \"train\"\n",
    "    out.loc[out.index.isin(g.index[va]), \"split3\"] = \"val\"\n",
    "    out.loc[out.index.isin(g.index[te]), \"split3\"] = \"test\"\n",
    "    return out\n",
    "\n",
    "subset = subset.groupby(\"identity\", group_keys=False).apply(split_one)\n",
    "\n",
    "# trim to exactly TARGET, dropping from train first (but keep ≥1 train per identity)\n",
    "def trim_to_target(df, target):\n",
    "    excess = len(df) - target\n",
    "    if excess <= 0: return df\n",
    "    train = df[df[\"split3\"]==\"train\"].copy()\n",
    "    keep_min = train.groupby(\"identity\").head(1).index\n",
    "    droppable = train.index.difference(keep_min)\n",
    "    drop_n = min(excess, len(droppable))\n",
    "    drop_idx = rng.choice(droppable, size=drop_n, replace=False) if drop_n>0 else []\n",
    "    return df.drop(index=drop_idx)\n",
    "\n",
    "subset = trim_to_target(subset, TARGET)\n",
    "\n",
    "# final labels 0..C-1\n",
    "id_set   = sorted(subset[\"identity\"].unique().tolist())\n",
    "id2label = {iid:i for i, iid in enumerate(id_set)}\n",
    "subset[\"label\"] = subset[\"identity\"].map(id2label).astype(int)\n",
    "\n",
    "train_df = subset[subset[\"split3\"]==\"train\"].reset_index(drop=True)\n",
    "val_df   = subset[subset[\"split3\"]==\"val\"].reset_index(drop=True)\n",
    "test_df  = subset[subset[\"split3\"]==\"test\"].reset_index(drop=True)\n",
    "NUM_CLASSES = len(id_set)\n",
    "\n",
    "def summarize(name, d):\n",
    "    if len(d)==0: return f\"{name}: n=0, ids=0\"\n",
    "    vc = d[\"identity\"].value_counts()\n",
    "    return f\"{name}: n={len(d):5d}, ids={d['identity'].nunique():4d}, min/max per id={int(vc.min())}/{int(vc.max())}\"\n",
    "\n",
    "print(\"Closed-set 50k split\")\n",
    "print(\"====================\")\n",
    "print(summarize(\"train\", train_df))\n",
    "print(summarize(\"val  \", val_df))\n",
    "print(summarize(\"test \", test_df))\n",
    "print(\"TOTAL:\", len(subset), \"| NUM_CLASSES:\", NUM_CLASSES)\n",
    "print(\"\\nExamples:\\n\", train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19bcf17f-5ffe-4504-b212-38cef5b6165e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train e1 0/625] loss=7.8652\n",
      "[train e1 200/625] loss=7.3322\n",
      "[train e1 400/625] loss=7.2510\n",
      "[train e1 600/625] loss=6.6466\n",
      "[val e1 0/79] loss=6.8549\n",
      "Epoch 01 | train acc 0.003 loss 7.3068 (68.9 ms/step) | val acc 0.013 loss 6.6644 (35.7 ms/step)\n",
      "  ↳ saved best checkpoint\n",
      "[train e2 0/625] loss=6.9300\n",
      "[train e2 200/625] loss=6.8705\n",
      "[train e2 400/625] loss=6.1190\n",
      "[train e2 600/625] loss=5.6395\n",
      "[val e2 0/79] loss=5.8170\n",
      "Epoch 02 | train acc 0.030 loss 6.4598 (66.4 ms/step) | val acc 0.063 loss 5.7003 (25.8 ms/step)\n",
      "  ↳ saved best checkpoint\n",
      "[train e3 0/625] loss=5.7379\n",
      "[train e3 200/625] loss=6.3021\n",
      "[train e3 400/625] loss=5.7125\n",
      "[train e3 600/625] loss=5.4056\n",
      "[val e3 0/79] loss=5.1611\n",
      "Epoch 03 | train acc 0.083 loss 5.8232 (66.5 ms/step) | val acc 0.139 loss 5.1255 (25.1 ms/step)\n",
      "  ↳ saved best checkpoint\n",
      "[train e4 0/625] loss=6.7336\n",
      "[train e4 200/625] loss=5.1390\n",
      "[train e4 400/625] loss=4.8711\n",
      "[train e4 600/625] loss=4.7649\n",
      "[val e4 0/79] loss=4.4126\n",
      "Epoch 04 | train acc 0.155 loss 5.3183 (66.5 ms/step) | val acc 0.212 loss 4.6630 (24.7 ms/step)\n",
      "  ↳ saved best checkpoint\n",
      "[train e5 0/625] loss=4.5807\n",
      "[train e5 200/625] loss=4.5127\n",
      "[train e5 400/625] loss=4.8741\n",
      "[train e5 600/625] loss=4.8727\n",
      "[val e5 0/79] loss=4.3020\n",
      "Epoch 05 | train acc 0.231 loss 4.9867 (66.5 ms/step) | val acc 0.278 loss 4.3084 (25.7 ms/step)\n",
      "  ↳ saved best checkpoint\n",
      "[train e6 0/625] loss=4.2296\n",
      "[train e6 200/625] loss=3.8374\n",
      "[train e6 400/625] loss=6.6036\n",
      "[train e6 600/625] loss=4.4562\n",
      "[val e6 0/79] loss=3.8921\n",
      "Epoch 06 | train acc 0.301 loss 4.6467 (66.4 ms/step) | val acc 0.372 loss 3.9189 (24.9 ms/step)\n",
      "  ↳ saved best checkpoint\n",
      "[train e7 0/625] loss=6.5815\n",
      "[train e7 200/625] loss=4.5387\n",
      "[train e7 400/625] loss=3.5686\n",
      "[train e7 600/625] loss=3.4184\n",
      "[val e7 0/79] loss=3.6738\n",
      "Epoch 07 | train acc 0.355 loss 4.4326 (66.4 ms/step) | val acc 0.417 loss 3.7281 (24.7 ms/step)\n",
      "  ↳ saved best checkpoint\n",
      "[train e8 0/625] loss=6.3973\n",
      "[train e8 200/625] loss=5.8852\n",
      "[train e8 400/625] loss=5.6810\n",
      "[train e8 600/625] loss=3.6497\n",
      "[val e8 0/79] loss=3.4370\n",
      "Epoch 08 | train acc 0.394 loss 4.2359 (66.5 ms/step) | val acc 0.471 loss 3.5060 (25.0 ms/step)\n",
      "  ↳ saved best checkpoint\n",
      "[train e9 0/625] loss=3.5308\n",
      "[train e9 200/625] loss=3.4139\n",
      "[train e9 400/625] loss=4.8736\n",
      "[train e9 600/625] loss=5.2138\n",
      "[val e9 0/79] loss=3.3515\n",
      "Epoch 09 | train acc 0.429 loss 4.1230 (66.4 ms/step) | val acc 0.492 loss 3.4508 (25.4 ms/step)\n",
      "  ↳ saved best checkpoint\n",
      "[train e10 0/625] loss=3.0719\n",
      "[train e10 200/625] loss=3.2898\n",
      "[train e10 400/625] loss=3.3683\n",
      "[train e10 600/625] loss=3.2533\n",
      "[val e10 0/79] loss=3.3158\n",
      "Epoch 10 | train acc 0.446 loss 4.0587 (66.5 ms/step) | val acc 0.504 loss 3.4055 (25.0 ms/step)\n",
      "  ↳ saved best checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_557672/2001149907.py:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(CKPTDIR / \"id_resnet18_best.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy: 0.5055 (best val: 0.5041)\n"
     ]
    }
   ],
   "source": [
    "# === Identity classification with ResNet18 (robust setup) ===\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os, time, torch, torch.nn as nn\n",
    "from torch import amp\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import numpy as np\n",
    "\n",
    "# ------------------ CONFIG ------------------\n",
    "ROOT      = Path(\"/home/anand.ha/NNDL/celeba\")\n",
    "IMG_DIR   = ROOT / \"data/raw\" / \"img_align_celeba\"\n",
    "OUTDIR    = ROOT / \"outputs\";     OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPTDIR   = ROOT / \"checkpoints\"; CKPTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE      = 160\n",
    "BATCH         = 64            # drop to 48/32 if OOM\n",
    "EPOCHS        = 10\n",
    "LR            = 3e-4\n",
    "WEIGHT_DECAY  = 1e-3          # stronger WD to reduce overfitting\n",
    "LABEL_SMOOTH  = 0.10\n",
    "USE_SAMPLER   = True          # class-balanced sampling for train\n",
    "MIXUP_ALPHA   = 0.2           # 0 to disable MixUp\n",
    "\n",
    "assert IMG_DIR.is_dir(), f\"Images folder missing: {IMG_DIR}\"\n",
    "assert len(train_df) and len(val_df) and len(test_df), \"Expected non-empty splits\"\n",
    "# --------------------------------------------\n",
    "\n",
    "# --------- Transforms (stronger augmentation) ---------\n",
    "train_tf = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.6, 1.0), ratio=(0.8, 1.25)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandAugment(num_ops=2, magnitude=9),\n",
    "    T.RandomGrayscale(p=0.10),\n",
    "    T.ColorJitter(0.15, 0.15, 0.15, 0.05),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "])\n",
    "eval_tf = T.Compose([\n",
    "    T.Resize(int(IMG_SIZE*1.15)),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# --------- Dataset ---------\n",
    "class IdDataset(Dataset):\n",
    "    def __init__(self, frame, img_dir, transform):\n",
    "        self.df = frame.reset_index(drop=True)\n",
    "        self.dir = Path(img_dir)\n",
    "        self.t   = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r  = self.df.iloc[i]\n",
    "        x  = Image.open(self.dir / r[\"image_id\"]).convert(\"RGB\")\n",
    "        x  = self.t(x)\n",
    "        y  = int(r[\"label\"])\n",
    "        return x, y\n",
    "\n",
    "def num_workers_default():\n",
    "    n = os.cpu_count() or 4\n",
    "    return max(2, min(8, n-2))\n",
    "\n",
    "# --------- Optional balanced sampler (train) ---------\n",
    "if USE_SAMPLER:\n",
    "    cls_counts = train_df['label'].value_counts().to_dict()\n",
    "    inv = {c: 1.0/v for c, v in cls_counts.items()}\n",
    "    sample_weights = train_df['label'].map(inv).astype(float).values\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    train_dl = DataLoader(IdDataset(train_df, IMG_DIR, train_tf),\n",
    "                          batch_size=BATCH, sampler=sampler, shuffle=False,\n",
    "                          num_workers=num_workers_default(), pin_memory=True,\n",
    "                          persistent_workers=True, prefetch_factor=4)\n",
    "else:\n",
    "    train_dl = DataLoader(IdDataset(train_df, IMG_DIR, train_tf),\n",
    "                          batch_size=BATCH, shuffle=True,\n",
    "                          num_workers=num_workers_default(), pin_memory=True,\n",
    "                          persistent_workers=True, prefetch_factor=4)\n",
    "\n",
    "val_dl  = DataLoader(IdDataset(val_df, IMG_DIR, eval_tf),\n",
    "                     batch_size=BATCH, shuffle=False,\n",
    "                     num_workers=num_workers_default(), pin_memory=True,\n",
    "                     persistent_workers=True, prefetch_factor=4)\n",
    "test_dl = DataLoader(IdDataset(test_df, IMG_DIR, eval_tf),\n",
    "                     batch_size=BATCH, shuffle=False,\n",
    "                     num_workers=num_workers_default(), pin_memory=True,\n",
    "                     persistent_workers=True, prefetch_factor=4)\n",
    "\n",
    "# --------- Model (ResNet18 + dropout head) ---------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "net.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.30),\n",
    "    nn.Linear(net.fc.in_features, NUM_CLASSES)\n",
    ")\n",
    "net.to(device)\n",
    "\n",
    "# --------- Optim, Loss, AMP, Scheduler ---------\n",
    "opt     = torch.optim.AdamW(net.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "sched   = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "scaler  = amp.GradScaler(\"cuda\") if device.type==\"cuda\" else None\n",
    "crit    = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "# --------- MixUp helpers ---------\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha <= 0: \n",
    "        return x, y, y, 1.0\n",
    "    # sample Beta(alpha, alpha)\n",
    "    lam = torch.distributions.Beta(alpha, alpha).sample().item()\n",
    "    lam = max(lam, 1.0 - lam)   # make it symmetric\n",
    "    idx = torch.randperm(x.size(0), device=x.device)\n",
    "    return lam * x + (1-lam) * x[idx], y, y[idx], lam\n",
    "\n",
    "def mixup_criterion(crit, pred, y_a, y_b, lam):\n",
    "    return lam * crit(pred, y_a) + (1 - lam) * crit(pred, y_b)\n",
    "\n",
    "# --------- Train/Eval loops ---------\n",
    "def run_epoch(loader, train: bool, epoch: int):\n",
    "    net.train(train)\n",
    "    tot = correct = 0\n",
    "    loss_sum = 0.0\n",
    "    t0 = time.time()\n",
    "    if train: opt.zero_grad(set_to_none=True)\n",
    "    for step, (xb, yb) in enumerate(loader):\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "\n",
    "        if train and MIXUP_ALPHA > 0:\n",
    "            xb_m, ya, yb2, lam = mixup_data(xb, yb, alpha=MIXUP_ALPHA)\n",
    "            with amp.autocast(\"cuda\", enabled=(device.type==\"cuda\")):\n",
    "                logits = net(xb_m)\n",
    "                loss   = mixup_criterion(crit, logits, ya, yb2, lam)\n",
    "        else:\n",
    "            with amp.autocast(\"cuda\", enabled=(device.type==\"cuda\")):\n",
    "                logits = net(xb)\n",
    "                loss   = crit(logits, yb)\n",
    "\n",
    "        if train:\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # metrics (for MixUp we still compute argmax on logits vs yb)\n",
    "        loss_sum += loss.item() * yb.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        tot += yb.size(0)\n",
    "\n",
    "        if step % 200 == 0:\n",
    "            print(f\"[{'train' if train else 'val'} e{epoch} {step}/{len(loader)}] loss={loss.item():.4f}\")\n",
    "\n",
    "    secs_per_step = (time.time() - t0) / max(1, len(loader))\n",
    "    return loss_sum / max(1, tot), correct / max(1, tot), secs_per_step\n",
    "\n",
    "# --------- Run training ---------\n",
    "best_val = 0.0\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc, tr_ms = run_epoch(train_dl, True,  ep)\n",
    "    va_loss, va_acc, va_ms = run_epoch(val_dl,   False, ep)\n",
    "    sched.step()\n",
    "    print(f\"Epoch {ep:02d} | \"\n",
    "          f\"train acc {tr_acc:.3f} loss {tr_loss:.4f} ({tr_ms*1000:.1f} ms/step) | \"\n",
    "          f\"val acc {va_acc:.3f} loss {va_loss:.4f} ({va_ms*1000:.1f} ms/step)\")\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save({\"epoch\": ep,\n",
    "                    \"state_dict\": net.state_dict(),\n",
    "                    \"val_acc\": va_acc,\n",
    "                    \"num_classes\": NUM_CLASSES},\n",
    "                   CKPTDIR / \"id_resnet18_best.pt\")\n",
    "        print(\"  ↳ saved best checkpoint\")\n",
    "\n",
    "# --------- Test (best checkpoint) ---------\n",
    "ck = torch.load(CKPTDIR / \"id_resnet18_best.pt\", map_location=device)\n",
    "net.load_state_dict(ck[\"state_dict\"])\n",
    "net.eval()\n",
    "\n",
    "tot = correct = 0\n",
    "with torch.no_grad(), amp.autocast(\"cuda\", enabled=(device.type==\"cuda\")):\n",
    "    for xb, yb in test_dl:\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        pred = net(xb).argmax(1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        tot += yb.size(0)\n",
    "\n",
    "test_acc = correct / max(1, tot)\n",
    "print(f\"TEST accuracy: {test_acc:.4f} (best val: {best_val:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaaad75-6892-45ed-8b4c-30ea57866e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0a288e-dedb-414d-ae5b-5d29e156bb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /home/anand.ha/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "100%|██████████| 83.3M/83.3M [00:00<00:00, 217MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train e1 0/834] loss=7.5181\n",
      "[train e1 200/834] loss=7.4540\n",
      "[train e1 400/834] loss=7.4751\n",
      "[train e1 600/834] loss=7.2060\n",
      "[train e1 800/834] loss=6.9843\n",
      "[val e1 0/105] loss=7.2391\n",
      "Epoch 01 | train 0.001/7.3942 | val 0.002/7.0520\n",
      "  ↳ saved best checkpoint\n",
      "[train e2 0/834] loss=7.0556\n",
      "[train e2 200/834] loss=6.8570\n",
      "[train e2 400/834] loss=6.7981\n",
      "[train e2 600/834] loss=6.7698\n",
      "[train e2 800/834] loss=6.4158\n",
      "[val e2 0/105] loss=6.7500\n",
      "Epoch 02 | train 0.004/6.6824 | val 0.006/6.5395\n",
      "  ↳ saved best checkpoint\n",
      "[train e3 0/834] loss=6.4990\n",
      "[train e3 200/834] loss=6.1251\n",
      "[train e3 400/834] loss=6.3072\n",
      "[train e3 600/834] loss=6.4014\n",
      "[train e3 800/834] loss=5.9691\n",
      "[val e3 0/105] loss=6.2585\n",
      "Epoch 03 | train 0.012/6.2235 | val 0.016/6.2595\n",
      "  ↳ saved best checkpoint\n",
      "[train e4 0/834] loss=5.9431\n",
      "[train e4 200/834] loss=6.0638\n",
      "[train e4 400/834] loss=5.7014\n",
      "[train e4 600/834] loss=5.4180\n",
      "[train e4 800/834] loss=5.4041\n",
      "[val e4 0/105] loss=5.4370\n",
      "Epoch 04 | train 0.038/5.6745 | val 0.056/5.4649\n",
      "  ↳ saved best checkpoint\n",
      "[train e5 0/834] loss=5.4077\n",
      "[train e5 200/834] loss=5.1565\n",
      "[train e5 400/834] loss=5.1443\n",
      "[train e5 600/834] loss=4.9095\n",
      "[train e5 800/834] loss=4.5138\n",
      "[val e5 0/105] loss=4.7410\n",
      "Epoch 05 | train 0.096/5.0409 | val 0.118/4.8924\n",
      "  ↳ saved best checkpoint\n",
      "[train e6 0/834] loss=4.4077\n",
      "[train e6 200/834] loss=4.7722\n",
      "[train e6 400/834] loss=4.4290\n",
      "[train e6 600/834] loss=4.3386\n",
      "[train e6 800/834] loss=3.8552\n",
      "[val e6 0/105] loss=4.1648\n",
      "Epoch 06 | train 0.185/4.4129 | val 0.203/4.3566\n",
      "  ↳ saved best checkpoint\n",
      "[train e7 0/834] loss=4.1569\n",
      "[train e7 200/834] loss=4.2738\n",
      "[train e7 400/834] loss=4.0217\n",
      "[train e7 600/834] loss=3.3847\n",
      "[train e7 800/834] loss=3.8898\n",
      "[val e7 0/105] loss=3.3977\n",
      "Epoch 07 | train 0.279/3.8761 | val 0.291/3.8487\n",
      "  ↳ saved best checkpoint\n",
      "[train e8 0/834] loss=3.7822\n",
      "[train e8 200/834] loss=3.8140\n",
      "[train e8 400/834] loss=3.0439\n",
      "[train e8 600/834] loss=3.2559\n",
      "[train e8 800/834] loss=3.2100\n",
      "[val e8 0/105] loss=3.2382\n",
      "Epoch 08 | train 0.381/3.3797 | val 0.379/3.4398\n",
      "  ↳ saved best checkpoint\n",
      "[train e9 0/834] loss=3.1864\n",
      "[train e9 200/834] loss=2.7668\n",
      "[train e9 400/834] loss=3.2544\n",
      "[train e9 600/834] loss=3.0586\n",
      "[train e9 800/834] loss=2.7487\n",
      "[val e9 0/105] loss=2.9605\n",
      "Epoch 09 | train 0.475/2.9827 | val 0.468/3.0933\n",
      "  ↳ saved best checkpoint\n",
      "[train e10 0/834] loss=2.8317\n",
      "[train e10 200/834] loss=3.2527\n",
      "[train e10 400/834] loss=2.4406\n",
      "[train e10 600/834] loss=2.4577\n",
      "[train e10 800/834] loss=2.8325\n",
      "[val e10 0/105] loss=2.8593\n",
      "Epoch 10 | train 0.549/2.6723 | val 0.499/2.9333\n",
      "  ↳ saved best checkpoint\n",
      "[train e11 0/834] loss=2.7823\n",
      "[train e11 200/834] loss=2.3939\n",
      "[train e11 400/834] loss=2.1786\n",
      "[train e11 600/834] loss=2.5363\n",
      "[train e11 800/834] loss=2.1338\n",
      "[val e11 0/105] loss=2.6373\n",
      "Epoch 11 | train 0.611/2.4267 | val 0.555/2.7273\n",
      "  ↳ saved best checkpoint\n",
      "[train e12 0/834] loss=2.2679\n",
      "[train e12 200/834] loss=1.7276\n",
      "[train e12 400/834] loss=2.0766\n",
      "[train e12 600/834] loss=1.9455\n",
      "[train e12 800/834] loss=2.1664\n",
      "[val e12 0/105] loss=2.4441\n",
      "Epoch 12 | train 0.658/2.2462 | val 0.578/2.5967\n",
      "  ↳ saved best checkpoint\n",
      "[train e13 0/834] loss=2.1519\n",
      "[train e13 200/834] loss=2.2922\n",
      "[train e13 400/834] loss=2.1422\n",
      "[train e13 600/834] loss=1.7519\n",
      "[train e13 800/834] loss=2.1142\n",
      "[val e13 0/105] loss=2.3429\n",
      "Epoch 13 | train 0.694/2.1113 | val 0.597/2.5254\n",
      "  ↳ saved best checkpoint\n",
      "[train e14 0/834] loss=2.0380\n",
      "[train e14 200/834] loss=1.9801\n",
      "[train e14 400/834] loss=2.1160\n",
      "[train e14 600/834] loss=2.0083\n",
      "[train e14 800/834] loss=2.2823\n",
      "[val e14 0/105] loss=2.3292\n",
      "Epoch 14 | train 0.717/2.0269 | val 0.614/2.4780\n",
      "  ↳ saved best checkpoint\n",
      "[train e15 0/834] loss=2.5645\n",
      "[train e15 200/834] loss=2.0435\n",
      "[train e15 400/834] loss=2.0039\n",
      "[train e15 600/834] loss=2.1265\n",
      "[train e15 800/834] loss=2.0834\n",
      "[val e15 0/105] loss=2.3508\n",
      "Epoch 15 | train 0.724/1.9922 | val 0.621/2.4695\n",
      "  ↳ saved best checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_557672/2564818027.py:123: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(CKPTDIR/\"id_resnet34_best.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy: 0.6179 (best val: 0.6213)\n"
     ]
    }
   ],
   "source": [
    "# === Identity classification with ResNet34 @ 224 (moderate regularization) ===\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os, time, torch, torch.nn as nn\n",
    "from torch import amp\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "ROOT    = Path(\"/home/anand.ha/NNDL/celeba\")\n",
    "IMG_DIR = ROOT / \"data/raw\" / \"img_align_celeba\"\n",
    "CKPTDIR = ROOT / \"checkpoints\"; CKPTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert IMG_DIR.is_dir()\n",
    "assert len(train_df) and len(val_df) and len(test_df)\n",
    "\n",
    "# ---- config\n",
    "IMG_SIZE = 224\n",
    "BATCH = 48            # try 64 if memory allows\n",
    "EPOCHS = 15\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 3e-4\n",
    "LABEL_SMOOTH = 0.05\n",
    "USE_SAMPLER = True    # class-balanced train sampler\n",
    "\n",
    "# ---- transforms (moderate)\n",
    "train_tf = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0), ratio=(0.85, 1.2)),\n",
    "    T.RandomHorizontalFlip(0.5),\n",
    "    T.RandAugment(num_ops=2, magnitude=6),\n",
    "    T.ColorJitter(0.10, 0.10, 0.10, 0.03),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    T.RandomErasing(p=0.15, scale=(0.02, 0.08), ratio=(0.3, 3.3)),\n",
    "])\n",
    "eval_tf = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# ---- dataset\n",
    "class IdDataset(Dataset):\n",
    "    def __init__(self, frame, img_dir, tfm):\n",
    "        self.df = frame.reset_index(drop=True)\n",
    "        self.dir = Path(img_dir); self.t = tfm\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        x = Image.open(self.dir / r[\"image_id\"]).convert(\"RGB\")\n",
    "        x = self.t(x)\n",
    "        return x, int(r[\"label\"])\n",
    "\n",
    "def nworkers(): \n",
    "    n = os.cpu_count() or 4\n",
    "    return max(2, min(8, n-2))\n",
    "\n",
    "if USE_SAMPLER:\n",
    "    counts = train_df[\"label\"].value_counts().to_dict()\n",
    "    weights = train_df[\"label\"].map({c:1.0/v for c,v in counts.items()}).astype(float).values\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "    train_dl = DataLoader(IdDataset(train_df, IMG_DIR, train_tf),\n",
    "                          batch_size=BATCH, sampler=sampler, shuffle=False,\n",
    "                          num_workers=nworkers(), pin_memory=True,\n",
    "                          persistent_workers=True, prefetch_factor=4)\n",
    "else:\n",
    "    train_dl = DataLoader(IdDataset(train_df, IMG_DIR, train_tf),\n",
    "                          batch_size=BATCH, shuffle=True,\n",
    "                          num_workers=nworkers(), pin_memory=True,\n",
    "                          persistent_workers=True, prefetch_factor=4)\n",
    "\n",
    "val_dl  = DataLoader(IdDataset(val_df, IMG_DIR, eval_tf),\n",
    "                     batch_size=BATCH, shuffle=False,\n",
    "                     num_workers=nworkers(), pin_memory=True,\n",
    "                     persistent_workers=True, prefetch_factor=4)\n",
    "test_dl = DataLoader(IdDataset(test_df, IMG_DIR, eval_tf),\n",
    "                     batch_size=BATCH, shuffle=False,\n",
    "                     num_workers=nworkers(), pin_memory=True,\n",
    "                     persistent_workers=True, prefetch_factor=4)\n",
    "\n",
    "# ---- model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "net.fc = nn.Sequential(nn.Dropout(0.15), nn.Linear(net.fc.in_features, NUM_CLASSES))\n",
    "net.to(device)\n",
    "\n",
    "# ---- optim/loss/sched\n",
    "opt   = torch.optim.AdamW(net.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n",
    "scaler= amp.GradScaler(\"cuda\") if device.type==\"cuda\" else None\n",
    "crit  = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "def run_epoch(loader, train, ep):\n",
    "    net.train(train)\n",
    "    tot=correct=0; loss_sum=0.0; t0=time.time()\n",
    "    if train: opt.zero_grad(set_to_none=True)\n",
    "    for step,(xb,yb) in enumerate(loader):\n",
    "        xb,yb = xb.to(device,non_blocking=True), yb.to(device,non_blocking=True)\n",
    "        with amp.autocast(\"cuda\", enabled=(device.type==\"cuda\")):\n",
    "            logits = net(xb); loss = crit(logits,yb)\n",
    "        if train:\n",
    "            if scaler: scaler.scale(loss).backward(); scaler.step(opt); scaler.update()\n",
    "            else: loss.backward(); opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "        loss_sum += loss.item()*yb.size(0)\n",
    "        pred = logits.argmax(1); correct += (pred==yb).sum().item(); tot += yb.size(0)\n",
    "        if step%200==0: print(f\"[{'train' if train else 'val'} e{ep} {step}/{len(loader)}] loss={loss.item():.4f}\")\n",
    "    return loss_sum/max(1,tot), correct/max(1,tot), (time.time()-t0)/max(1,len(loader))\n",
    "\n",
    "best_val=0.0\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss,tr_acc,_ = run_epoch(train_dl, True,  ep)\n",
    "    va_loss,va_acc,_ = run_epoch(val_dl,   False, ep)\n",
    "    sched.step()\n",
    "    print(f\"Epoch {ep:02d} | train {tr_acc:.3f}/{tr_loss:.4f} | val {va_acc:.3f}/{va_loss:.4f}\")\n",
    "    if va_acc>best_val:\n",
    "        best_val=va_acc\n",
    "        torch.save({\"epoch\":ep,\"state_dict\":net.state_dict(),\"val_acc\":va_acc,\"num_classes\":NUM_CLASSES},\n",
    "                   CKPTDIR/\"id_resnet34_best.pt\")\n",
    "        print(\"  ↳ saved best checkpoint\")\n",
    "\n",
    "ck = torch.load(CKPTDIR/\"id_resnet34_best.pt\", map_location=device)\n",
    "net.load_state_dict(ck[\"state_dict\"]); net.eval()\n",
    "tot=correct=0\n",
    "with torch.no_grad(), amp.autocast(\"cuda\", enabled=(device.type==\"cuda\")):\n",
    "    for xb,yb in test_dl:\n",
    "        xb,yb = xb.to(device,non_blocking=True), yb.to(device,non_blocking=True)\n",
    "        pred = net(xb).argmax(1); correct += (pred==yb).sum().item(); tot += yb.size(0)\n",
    "print(f\"TEST accuracy: {correct/max(1,tot):.4f} (best val: {best_val:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd9eeee-c6f4-4baa-b858-853c7e85a111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /home/anand.ha/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 90.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train e1 0/625] loss=7.4737\n",
      "[train e1 200/625] loss=7.3040\n",
      "[train e1 400/625] loss=6.8235\n",
      "[train e1 600/625] loss=6.1865\n",
      "[val e1 0/79] loss=5.7852\n",
      "Epoch 01 | train 0.014/6.9203 | val 0.080/5.6904\n",
      "  ↳ saved best checkpoint\n",
      "[train e2 0/625] loss=5.6545\n",
      "[train e2 200/625] loss=5.6668\n",
      "[train e2 400/625] loss=5.2956\n",
      "[train e2 600/625] loss=4.9763\n",
      "[val e2 0/79] loss=4.3553\n",
      "Epoch 02 | train 0.114/5.3700 | val 0.264/4.4909\n",
      "  ↳ saved best checkpoint\n",
      "[train e3 0/625] loss=4.3242\n",
      "[train e3 200/625] loss=4.5020\n",
      "[train e3 400/625] loss=4.1200\n",
      "[train e3 600/625] loss=4.3903\n",
      "[val e3 0/79] loss=3.5428\n",
      "Epoch 03 | train 0.265/4.4107 | val 0.415/3.7622\n",
      "  ↳ saved best checkpoint\n",
      "[train e4 0/625] loss=3.8484\n",
      "[train e4 200/625] loss=3.7023\n",
      "[train e4 400/625] loss=3.3510\n",
      "[train e4 600/625] loss=3.6288\n",
      "[val e4 0/79] loss=3.1287\n",
      "Epoch 04 | train 0.397/3.7501 | val 0.520/3.3204\n",
      "  ↳ saved best checkpoint\n",
      "[train e5 0/625] loss=3.4909\n",
      "[train e5 200/625] loss=3.1059\n",
      "[train e5 400/625] loss=3.3256\n",
      "[train e5 600/625] loss=3.1383\n",
      "[val e5 0/79] loss=2.8612\n",
      "Epoch 05 | train 0.504/3.2929 | val 0.577/3.0650\n",
      "  ↳ saved best checkpoint\n",
      "[train e6 0/625] loss=3.1321\n",
      "[train e6 200/625] loss=2.6335\n",
      "[train e6 400/625] loss=2.9170\n",
      "[train e6 600/625] loss=3.0941\n",
      "[val e6 0/79] loss=2.6816\n",
      "Epoch 06 | train 0.583/2.9693 | val 0.628/2.8521\n",
      "  ↳ saved best checkpoint\n",
      "[train e7 0/625] loss=2.6962\n",
      "[train e7 200/625] loss=2.8323\n",
      "[train e7 400/625] loss=2.5717\n",
      "[train e7 600/625] loss=2.9219\n",
      "[val e7 0/79] loss=2.5590\n",
      "Epoch 07 | train 0.645/2.7232 | val 0.661/2.7261\n",
      "  ↳ saved best checkpoint\n",
      "[train e8 0/625] loss=2.7138\n",
      "[train e8 200/625] loss=2.6215\n",
      "[train e8 400/625] loss=2.5121\n",
      "[train e8 600/625] loss=2.6105\n",
      "[val e8 0/79] loss=2.4982\n",
      "Epoch 08 | train 0.696/2.5312 | val 0.681/2.6293\n",
      "  ↳ saved best checkpoint\n",
      "[train e9 0/625] loss=2.4401\n",
      "[train e9 200/625] loss=2.6987\n",
      "[train e9 400/625] loss=2.3983\n",
      "[train e9 600/625] loss=2.3934\n",
      "[val e9 0/79] loss=2.4872\n",
      "Epoch 09 | train 0.735/2.3842 | val 0.698/2.5600\n",
      "  ↳ saved best checkpoint\n",
      "[train e10 0/625] loss=2.2030\n",
      "[train e10 200/625] loss=2.1018\n",
      "[train e10 400/625] loss=2.3250\n",
      "[train e10 600/625] loss=2.2536\n",
      "[val e10 0/79] loss=2.4055\n",
      "Epoch 10 | train 0.767/2.2736 | val 0.710/2.5118\n",
      "  ↳ saved best checkpoint\n",
      "[train e11 0/625] loss=2.0078\n",
      "[train e11 200/625] loss=2.1325\n",
      "[train e11 400/625] loss=2.2727\n",
      "[train e11 600/625] loss=2.1876\n",
      "[val e11 0/79] loss=2.3439\n",
      "Epoch 11 | train 0.794/2.1759 | val 0.720/2.4728\n",
      "  ↳ saved best checkpoint\n",
      "[train e12 0/625] loss=2.2547\n",
      "[train e12 200/625] loss=2.1462\n",
      "[train e12 400/625] loss=2.1096\n",
      "[train e12 600/625] loss=1.9303\n",
      "[val e12 0/79] loss=2.3372\n",
      "Epoch 12 | train 0.810/2.1137 | val 0.727/2.4543\n",
      "  ↳ saved best checkpoint\n",
      "[train e13 0/625] loss=2.0366\n",
      "[train e13 200/625] loss=1.9616\n",
      "[train e13 400/625] loss=2.2777\n",
      "[train e13 600/625] loss=2.2235\n",
      "[val e13 0/79] loss=2.3239\n",
      "Epoch 13 | train 0.823/2.0705 | val 0.730/2.4415\n",
      "  ↳ saved best checkpoint\n",
      "[train e14 0/625] loss=1.8721\n",
      "[train e14 200/625] loss=2.1856\n",
      "[train e14 400/625] loss=1.8366\n",
      "[train e14 600/625] loss=2.2824\n",
      "[val e14 0/79] loss=2.3539\n",
      "Epoch 14 | train 0.832/2.0451 | val 0.729/2.4324\n",
      "[train e15 0/625] loss=1.8184\n",
      "[train e15 200/625] loss=1.9402\n",
      "[train e15 400/625] loss=2.0814\n",
      "[train e15 600/625] loss=1.9928\n",
      "[val e15 0/79] loss=2.3506\n",
      "Epoch 15 | train 0.834/2.0317 | val 0.730/2.4286\n",
      "  ↳ saved best checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_557672/2211948592.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ck = torch.load(CKPTDIR/\"id_efficientnet_b0_best.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST accuracy: 0.7295 (best val: 0.7303)\n"
     ]
    }
   ],
   "source": [
    "# === Identity classification on CelebA subset with EfficientNet-B0 ===\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import os, time, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch import amp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "# --- paths (adjust if needed)\n",
    "ROOT    = Path(\"/home/anand.ha/NNDL/celeba\")\n",
    "IMG_DIR = ROOT / \"data/raw\" / \"img_align_celeba\"\n",
    "OUTDIR  = ROOT / \"outputs\"; OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPTDIR = ROOT / \"checkpoints\"; CKPTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# assumes you already have train_df, val_df, test_df, NUM_CLASSES built\n",
    "assert len(train_df) and len(val_df) and len(test_df), \"Expected non-empty splits\"\n",
    "\n",
    "# --- transforms\n",
    "IMG_SIZE = 160\n",
    "train_tf = T.Compose([\n",
    "    T.RandomResizedCrop(IMG_SIZE, scale=(0.6, 1.0), ratio=(0.8, 1.25)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandAugment(num_ops=2, magnitude=9),\n",
    "    T.RandomGrayscale(p=0.10),\n",
    "    T.ColorJitter(0.15, 0.15, 0.15, 0.05),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    T.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3))\n",
    "])\n",
    "\n",
    "eval_tf = T.Compose([\n",
    "    T.Resize(int(IMG_SIZE*1.15)),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# --- dataset class\n",
    "class IdDataset(Dataset):\n",
    "    def __init__(self, frame, img_dir, transform):\n",
    "        self.df = frame.reset_index(drop=True)\n",
    "        self.dir = Path(img_dir); self.t = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        x = Image.open(self.dir / r[\"image_id\"]).convert(\"RGB\")\n",
    "        x = self.t(x)\n",
    "        y = int(r[\"label\"])\n",
    "        return x, y\n",
    "\n",
    "def num_workers_default():\n",
    "    n = os.cpu_count() or 4\n",
    "    return max(2, min(8, n-2))\n",
    "\n",
    "BATCH = 64\n",
    "train_dl = DataLoader(IdDataset(train_df, IMG_DIR, train_tf),\n",
    "                      batch_size=BATCH, shuffle=True,\n",
    "                      num_workers=num_workers_default(), pin_memory=True,\n",
    "                      persistent_workers=True, prefetch_factor=4)\n",
    "val_dl   = DataLoader(IdDataset(val_df,   IMG_DIR, eval_tf),\n",
    "                      batch_size=BATCH, shuffle=False,\n",
    "                      num_workers=num_workers_default(), pin_memory=True,\n",
    "                      persistent_workers=True, prefetch_factor=4)\n",
    "test_dl  = DataLoader(IdDataset(test_df,  IMG_DIR, eval_tf),\n",
    "                      batch_size=BATCH, shuffle=False,\n",
    "                      num_workers=num_workers_default(), pin_memory=True,\n",
    "                      persistent_workers=True, prefetch_factor=4)\n",
    "\n",
    "# --- model (EfficientNet-B0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "net.classifier[1] = nn.Sequential(\n",
    "    nn.Dropout(p=0.4),\n",
    "    nn.Linear(net.classifier[1].in_features, NUM_CLASSES)\n",
    ")\n",
    "net.to(device)\n",
    "\n",
    "# --- optimizer/loss/amp\n",
    "opt = torch.optim.AdamW(net.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=15)\n",
    "scaler = amp.GradScaler(\"cuda\") if device.type==\"cuda\" else None\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "# --- training loop\n",
    "def run_epoch(loader, train: bool, epoch: int):\n",
    "    net.train(train)\n",
    "    tot, correct, loss_sum = 0, 0, 0.0\n",
    "    if train: opt.zero_grad(set_to_none=True)\n",
    "    t0 = time.time()\n",
    "    for step, (xb, yb) in enumerate(loader):\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        with amp.autocast(\"cuda\", enabled=(device.type==\"cuda\")):\n",
    "            logits = net(xb)\n",
    "            loss   = criterion(logits, yb)\n",
    "        if train:\n",
    "            if scaler:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt); scaler.update()\n",
    "            else:\n",
    "                loss.backward(); opt.step()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "        loss_sum += loss.item() * yb.size(0)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        tot += yb.size(0)\n",
    "        if step % 200 == 0:\n",
    "            print(f\"[{'train' if train else 'val'} e{epoch} {step}/{len(loader)}] loss={loss.item():.4f}\")\n",
    "    sec = time.time()-t0\n",
    "    return loss_sum/max(1,tot), correct/max(1,tot), sec/len(loader)\n",
    "\n",
    "# --- train for 15 epochs\n",
    "best_val = 0.0\n",
    "EPOCHS = 15\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc, tr_ms = run_epoch(train_dl, True,  ep)\n",
    "    va_loss, va_acc, va_ms = run_epoch(val_dl,   False, ep)\n",
    "    sched.step()\n",
    "    print(f\"Epoch {ep:02d} | train {tr_acc:.3f}/{tr_loss:.4f} | val {va_acc:.3f}/{va_loss:.4f}\")\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save({\"epoch\":ep, \"state_dict\":net.state_dict(), \"val_acc\":va_acc,\n",
    "                    \"num_classes\": NUM_CLASSES},\n",
    "                   CKPTDIR/\"id_efficientnet_b0_best.pt\")\n",
    "        print(\"  ↳ saved best checkpoint\")\n",
    "\n",
    "# --- test eval\n",
    "ck = torch.load(CKPTDIR/\"id_efficientnet_b0_best.pt\", map_location=device)\n",
    "net.load_state_dict(ck[\"state_dict\"]); net.eval()\n",
    "tot, correct = 0, 0\n",
    "with torch.no_grad(), amp.autocast(\"cuda\", enabled=(device.type==\"cuda\")):\n",
    "    for xb, yb in test_dl:\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        pred = net(xb).argmax(1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        tot += yb.size(0)\n",
    "test_acc = correct / max(1, tot)\n",
    "print(f\"TEST accuracy: {test_acc:.4f} (best val: {best_val:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc99bf88-840d-41cc-99a8-952f90114734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_557672/90107654.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(CKPT_PATH, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted identity label index: 1568\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths (edit IMG_PATH as you like) ===\n",
    "CKPT_PATH = Path(\"/home/anand.ha/NNDL/celeba/checkpoints/id_efficientnet_b0_best.pt\")\n",
    "IMG_PATH  = Path(\"/home/anand.ha/NNDL/celeba/data/raw/img_align_celeba/000098.jpg\")  # change to any image\n",
    "\n",
    "# === Load checkpoint first to get NUM_CLASSES if available ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt = torch.load(CKPT_PATH, map_location=device)\n",
    "num_classes = ckpt.get(\"num_classes\", 666)  # fallback if not saved\n",
    "\n",
    "# === Build model EXACTLY like training ===\n",
    "net = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "in_features = net.classifier[1].in_features\n",
    "# training used: classifier[1] = Sequential(Dropout, Linear)\n",
    "net.classifier[1] = nn.Sequential(\n",
    "    nn.Dropout(p=0.4),\n",
    "    nn.Linear(in_features, num_classes)\n",
    ")\n",
    "net.to(device)\n",
    "\n",
    "# Load weights (keep strict=True since we now match the arch)\n",
    "net.load_state_dict(ckpt[\"state_dict\"])\n",
    "net.eval()\n",
    "\n",
    "# === Preprocessing (same as eval_tf) ===\n",
    "IMG_SIZE = 160\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE * 1.15)),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# === Load image and predict ===\n",
    "img = Image.open(IMG_PATH).convert(\"RGB\")\n",
    "x = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = net(x)\n",
    "    pred_class = logits.argmax(dim=1).item()\n",
    "\n",
    "print(f\"Predicted identity label index: {pred_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57672bae-4d84-4684-a983-97bf1a4f5717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (celeba)",
   "language": "python",
   "name": "celeba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
